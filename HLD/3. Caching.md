### Agenda
1. Caching
2. Caching Vs Backup
3. Caching Types
   * In-Memory Caching
   * In-Browser Caching
   * Distributed Caching
   * Content Delivery Network (CDN) Caching
   * Application-Level Caching
   * Global Caching
4. Problems with Caching
5. Solutions to Caching Consistency Problems   
     *  Time-to-Live (TTL)
     * Write Strategies
       * Write-Through Cache
       * Write-Around Cache
       * Write-Back (Write-Behind) Cache
6. Solution of Cache Size Limit
       * Cache Eviction Policies
        * FIFO (First In, First Out)
            * LRU (Least Recently Used)
            * LIFO (Last In, First Out)
            * LFU (Least Frequently Used)


### Caching
        Storing of copy data at some another space so that next time when same data is requested it can be fetched from cache instead of original data source which is time consuming.
        It can be done on same machine or different machine.

#### Caching Vs Backup

| Aspect           | Caching                                                   | Backup                                                       |
|------------------|-----------------------------------------------------------|--------------------------------------------------------------|
| Purpose          | Improve performance and reduce latency                    | Data recovery and protection                                 |
| Data Freshness   | Typically stores recent or frequently accessed data       | Stores complete data snapshots at  specific points in time   |
| Storage Location | Can be on the same machine or separate cache servers      | Usually stored on separate storage systems or cloud services |
| Data Volume      | Generally smaller, focused on frequently accessed data    | Can be large, encompassing entire datasets                   |
| Access Speed     | Fast access for quick retrieval                           | Slower access, optimized for durability                      |
| Update Frequency | Frequently updated to reflect current data                | Updated periodically, often less frequently                  |
| Data Retention   | Short-term storage, data may be evicted                   | Long-term storage, data is preserved for recovery            |
| Use Cases        | Web applications, databases, content delivery networks    | Disaster recovery, archival, compliance                      |
| Examples         | In-memory caches (Redis, Memcached), CDN caches           | Backup solutions (AWS Backup, Veeam, Acronis)                |
| Frequency        | Real-time or near real-time                               | Scheduled or on-demand                                       |
| Data Consistency | May allow stale data for performance                      | Ensures data integrity and consistency                       |
| Cost             | Generally lower cost due to smaller storage needs         | Can be higher due to larger storage requirements             |
| Complexity       | Relatively simple to implement and manage                 | Can be complex, especially for large datasets                |
| Recovery Time    | Immediate access to cached data                           | May require time to restore data from backup                 |
| Data Type        | Frequently accessed data                                  | Complete datasets or critical files                          |
| Security         | May have basic security measures                          | Often includes robust security and encryption                |
| Management       | Typically managed by application or system administrators | Managed by IT or backup specialists                          |
| Scalability      | Easily scalable to handle increased load                  | Scalability depends on backup infrastructure                 |
| Data Format      | Often unstructured or semi-structured data                | Can include structured, unstructured, or binary data         |

#### Caching Types:
1. In-Memory Caching - Data is stored in RAM for ultra-fast access.(DNS Ipaddress,Google Typehead search suggestions)
2. In-Browser Caching - Web browsers store static resources locally to speed up page load times (e.g., HTML, CSS, JavaScript files).
2. Distributed Caching - Data is stored across multiple servers to improve scalability and fault tolerance (e.g., Amazon ElastiCache, Apache Ignite).
3. Content Delivery Network (CDN) Caching - Static content is cached at edge locations to reduce latency for users (e.g., Cloudflare, Akamai).
    * If application server is accessing the data then we don't use CDN since request will come from server only not users.
    * CDN gives good User Experience
    * Reduces Latency
    * CDN is expensive
4. Application-Level Caching - Specific data or computations are cached within the application logic with additional metadata. (e.g., memoization in functions).
5. Global Caching - Caching at centralised location to be accessed by multiple applications or services. ex(Redis)

Problem with caching:
* **Cache Invalidation** - Ensuring that cached data is updated or removed when the underlying data changes.
* **Cache Consistency** - Maintaining consistency between the cache and the original data source.
* **Cache Eviction** - Deciding which data to remove from the cache when it reaches its capacity.
* Cache Stampede - Preventing multiple requests from overwhelming the cache when a popular item expires.
* Latency - Network latency can impact the performance of distributed caches.

#### Solution of cache consistency:
1. Time-to-Live (TTL) -

* Cached data is assigned an expiration time after which it is invalidated.
* After specific time period data will be removed from cache automatically or mark it invalid.
* It will gain synchronicity after specific time period when new request come and data is fetched from original data source.
* If TTL is 1min, that means data will be valid for 1 min only after that cache will not allow to red that data.
* Suitable for data that changes infrequently.

Pros:
* Simple to implement.
* Reduces stale data issues.
* Data read is fast as its read from cache first then DB.

Cons:
* May lead to unnecessary cache misses if data is still valid.
* Not suitable for data that changes frequently.

When To use:
* When speed have more priority over consistency. Like in news feed application we can show slightly old data.
* When data changes infrequently.
* When some staleness is acceptable.
* When simplicity is a priority.

Write Strategies:

2.a.    **Write-Through Cache** -
* Data is written to both the cache and the underlying data store simultaneously.
* Data will write in cache first then in original data source. and mark success only after both operation is successful.
* Ensures data consistency between cache and original data source.
* Suitable for applications where data consistency is critical.

Pros:
* Ensures strong consistency between cache and data store.
* Simplicity in implementation.

Cons:
* Higher write latency due to dual writes.

When to Use :
* When data consistency is critical.
* When write latency is acceptable.
* When simplicity is a priority.

When Not to Use:

* When write performance is critical.
* When dealing with high write loads.
* CDN (Not possible to update Image at all Cache first the original source) and In Browing caching (same not possble to update data at all client browser Cache first)

Real applications: Banking systems, Inventory management systems.

2.b.    **Write-Around Cache** - Data is written to the Database directly, bypassing the cache done. The cache is only updated on read operations. using TTL approach.

Pros:
* Reduces write load on the cache.
* Suitable for write-heavy workloads where data is infrequently read.

Cons:
* May lead to cache misses on subsequent reads.
* Increased read latency for uncached data.

When to Use:
* When write load is high and read load is low.
* When data is infrequently read after being written.
* When cache size is limited.
* Real applications: Logging systems, Analytics data storage.

2.c.    **Write-Back (Write-Behind) Cache** - Data is written to the cache Done and then asynchronously to the underlying data store. Sync Db at every xmin
Pros:
* Improves write performance by reducing latency.
* Suitable for write-heavy workloads.
* Cache is always latest data.
  Cons:
* Risk of data loss if cache fails before data is written to the data store.
* Increased complexity in implementation.
* Data consistency challenges.

When to Use:
* When write performance is critical.
* When eventual consistency is acceptable.
* When dealing with high write loads.

Real Application - Host star live view counting system

create a table having 6 column and 4 rows showing Strategy, Description,Speed of write,speed of read,consistency,usecase

| Strategy      | Description                                                               | Speed of Write | Speed of Read | Consistency         | Use Case                               |
|---------------|---------------------------------------------------------------------------|------------|---------------|---------------------|----------------------------------------|
| TTL           | Application checks cache first; if data is not found, retrieves from DB   | Moderate   | Fast          | Eventual            | Frequently accessed data               |
| Read-Through  | Cache automatically loads data from DB on a cache miss                    | Fast       | Fast          |                     | Read-heavy workloads                   |
| Write-Through | Data is written to both cache and then DB simultaneously and mark as done | Slow       | Fast          | Strong              | Data consistency critical applications |
| Write-Around  | Data is written to Database.                                              | Moderate   | Moderate      | Eventual            |                                        |
| Write-Back    | Data is written to cache.                                                 | Fast       | Fast          | Eventual(Data lost) | Write-heavy workloads view count       |   

#### Solution of cache size limit:
Cache Eviction Policies - Implementing strategies to determine which data to remove from the cache when it reaches its capacity.

Algorithms:
* FIFO (First In, First Out) - Removes the oldest items first.
* LRU (Least Recently Used) - Removes the least recently accessed items first. Best and always used
* LIFO (Last In, First Out) - Removes the most recently added items first.
* LFU (Least Frequently Used) - Removes the least frequently accessed items first. In case of Tie apply FIFO
   

