### Agenda
1. [Caching](#caching)
2. [Caching Vs Backup](#caching-vs-backup)
3. [Caching Types](#caching-types)
   * In-Memory Caching
   * In-Browser Caching
   * Distributed Caching
   * Content Delivery Network (CDN) Caching
   * Application-Level Caching
   * Global Caching
4. Problems with Caching
5. [Solutions to Caching Consistency Problems](#solution-of-cache-consistency)   
     *  Time-to-Live (TTL)
     * Write Strategies
       * Write-Through Cache
       * Write-Around Cache
       * Write-Back (Write-Behind) Cache
6. [Solution of Cache Size Limit](#solution-of-cache-size-limit)   
   * FIFO (First In, First Out)
   * LRU (Least Recently Used)
   * LIFO (Last In, First Out)
   * LFU (Least Frequently Used)

Note: Reading is always from Cache first then DB

### Caching
        Storing of copy data at some another space so that next time when same data is requested it can be fetched from cache instead of original data source which is time consuming.
        It can be done on same machine or different machine.


Type of caching:
1. Client-Side Caching - Data is stored on the client device (browser or application) to reduce the need for repeated requests to the server.
2. Server-Side Caching - Data is stored on the server to quickly serve repeated requests without reprocessing.
3. Distributed Caching - Data is stored across multiple servers or nodes to improve scalability and fault tolerance.



#### Caching Vs Backup

| Aspect           | Caching                                                   | Backup                                                       |
|------------------|-----------------------------------------------------------|--------------------------------------------------------------|
| Purpose          | Improve performance and reduce latency                    | Data recovery and protection                                 |
| Data Freshness   | Typically stores recent or frequently accessed data       | Stores complete data snapshots at  specific points in time   |
| Storage Location | Can be on the same machine or separate cache servers      | Usually stored on separate storage systems or cloud services |
| Data Volume      | Generally smaller, focused on frequently accessed data    | Can be large, encompassing entire datasets                   |
| Access Speed     | Fast access for quick retrieval                           | Slower access, optimized for durability                      |
| Update Frequency | Frequently updated to reflect current data                | Updated periodically, often less frequently                  |
| Data Retention   | Short-term storage, data may be evicted                   | Long-term storage, data is preserved for recovery            |
| Use Cases        | Web applications, databases, content delivery networks    | Disaster recovery, archival, compliance                      |
| Examples         | In-memory caches (Redis, Memcached), CDN caches           | Backup solutions (AWS Backup, Veeam, Acronis)                |
| Frequency        | Real-time or near real-time                               | Scheduled or on-demand                                       |
| Data Consistency | May allow stale data for performance                      | Ensures data integrity and consistency                       |
| Cost             | Generally lower cost due to smaller storage needs         | Can be higher due to larger storage requirements             |
| Complexity       | Relatively simple to implement and manage                 | Can be complex, especially for large datasets                |
| Recovery Time    | Immediate access to cached data                           | May require time to restore data from backup                 |
| Data Type        | Frequently accessed data                                  | Complete datasets or critical files                          |
| Security         | May have basic security measures                          | Often includes robust security and encryption                |
| Management       | Typically managed by application or system administrators | Managed by IT or backup specialists                          |
| Scalability      | Easily scalable to handle increased load                  | Scalability depends on backup infrastructure                 |
| Data Format      | Often unstructured or semi-structured data                | Can include structured, unstructured, or binary data         |


#### Caching Types:
1. **In-Memory Caching** - Data is stored in RAM for ultra-fast access.(DNS Ipaddress,Google Typehead search suggestions)
2. **In-Browser Caching** - Web browsers store static resources locally to speed up page load times (e.g., HTML, CSS, JavaScript files).
2. **Distributed Caching** - Data is stored across multiple servers to improve scalability and fault tolerance (e.g., Amazon ElastiCache, Apache Ignite).
3. **Content Delivery Network (CDN) Caching** - Static content is cached at edge locations to reduce latency for users (e.g., Cloudflare, Akamai).
    * If application server is accessing the data then we don't use CDN since request will come from server only not users.
    * CDN gives good User Experience
    * Reduces Latency
    * CDN is expensive
4. **Application-Level Caching** - Specific data or computations are cached within the application logic with additional metadata. (e.g., memoization in functions).
5. **Global Caching** - Caching at centralised location to be accessed by multiple applications or services. ex(Redis)

Problem with caching:
* **Cache Invalidation** - Ensuring that cached data is updated or removed when the underlying data changes.
* **Cache Consistency** - Maintaining consistency between the cache and the original data source.
* **Cache Eviction** - Deciding which data to remove from the cache when it reaches its capacity.
* **Cache Stampede** - Preventing multiple requests from overwhelming the cache when a popular item expires.
* **Latency** - Network latency can impact the performance of distributed caches.

### Solution of cache consistency:
#### 1. Time-to-Live (TTL) -

* Cached data is assigned an expiration time after which it is invalidated.
* After specific time period data will be removed from cache automatically or mark it invalid.
* It will gain synchronicity after specific time period when new request come and data is fetched from original data source.
* If TTL is 1min, that means data will be valid for 1 min only after that cache will not allow to red that data.
* Suitable for data that changes infrequently.

**Pros**:
* Simple to implement.
* Reduces stale data issues.
* Data read is fast as its read from cache first then DB.

**Cons**:
* May lead to unnecessary cache misses if data is still valid.
* Not suitable for data that changes frequently.

**When To use**:
* When speed have more priority over consistency. Like in news feed application we can show slightly old data.
* When data changes infrequently.
* When some staleness is acceptable.
* When simplicity is a priority.

#### 2. Write Strategies:

2.a.    **Write-Through Cache** -
* Data is written to both the cache and the underlying data store simultaneously.
* Data will write in cache first then in original data source. and mark success only after both operation is successful.
* Ensures data consistency between cache and original data source.
* Suitable for applications where data consistency is critical.


**Pros**:
* Ensures strong consistency between cache and data store.
* Simplicity in implementation.

**Cons**:
* Higher write latency due to dual writes.

**When to Use** :
* When data consistency is critical.
* When write latency is acceptable.
* When simplicity is a priority.

**When Not to Use**:

* When write performance is critical.
* When dealing with high write loads.
* CDN (Not possible to update Image at all Cache first the original source) and In Browing caching (same not possble to update data at all client browser Cache first)

**Real applications**: Banking systems, Inventory management systems.

2.b.    **Write-Around Cache** - Data is written to the Database directly, bypassing the cache done. The cache is only updated on read operations. using TTL approach.

**Pros**:
* Reduces write load on the cache.
* Suitable for write-heavy workloads where data is infrequently read.

**Cons**:
* May lead to cache misses on subsequent reads.
* Increased read latency for uncached data.

**When to Use**:
* When write load is high and read load is low.
* When data is infrequently read after being written.
* When cache size is limited.
* Real applications: Logging systems, Analytics data storage.

2.c.    **Write-Back (Write-Behind) Cache** - Data is written to the cache Done and then asynchronously to the underlying data store. Sync Db at every x min

**Pros**:
* Improves write performance by reducing latency.
* Suitable for write-heavy workloads.
* Cache is always latest data.

**Cons**:
* Risk of data loss if cache fails before data is written to the data store.
* Increased complexity in implementation.
* Data consistency challenges.

**When to Use**:
* When write performance is critical.
* When eventual consistency is acceptable.
* When dealing with high write loads.

**Real Application** - Host star live view counting system

Summary:

| Strategy      | Description                                                               | Speed of Write | Speed of Read | Consistency         | Use Case                               |
|---------------|---------------------------------------------------------------------------|------------|---------------|---------------------|----------------------------------------|
| TTL           | Application checks cache first; if data is not found, retrieves from DB   | Moderate   | Fast          | Eventual            | Frequently accessed data               |
| Read-Through  | Cache automatically loads data from DB on a cache miss                    | Fast       | Fast          |                     | Read-heavy workloads                   |
| Write-Through | Data is written to both cache and then DB simultaneously and mark as done | Slow       | Fast          | Strong              | Data consistency critical applications |
| Write-Around  | Data is written to Database.                                              | Moderate   | Moderate      | Eventual            |                                        |
| Write-Back    | Data is written to cache.                                                 | Fast       | Fast          | Eventual(Data lost) | Write-heavy workloads view count       |   

#### Solution of cache size limit:

Cache Eviction Policies - Implementing strategies to determine which data to remove from the cache when it reaches its capacity.

**Algorithms**:
* **FIFO (First In, First Out)** - Removes the oldest items first.

When to use:
* When data access patterns are uniform.
* When simplicity is a priority.

Real Applications:
* Queue management systems.
* Print spooling systems.
* Task scheduling systems.
* Operating system page replacement algorithms.
* Networking devices for packet buffering.
* Message queuing systems.
* Web caching for static content.
* File system caching.
* Database buffer management.
* Embedded systems with limited memory.
* Multimedia streaming applications.
* Gaming applications for resource management.
* IoT devices for data caching.
* Content delivery networks (CDNs) for cache management.
* Mobile applications for offline data storage.
* Email clients for caching messages.
* Document editing applications for autosave features.
* Version control systems for caching file versions.
* Data analytics platforms for intermediate result caching.



* **LRU (Least Recently Used)** - Removes the least recently accessed items first. Best and always used

When to use:
* When recently accessed data is more likely to be accessed again.
* When optimizing for temporal locality.

Real Applications: 
* Database caching
* Web caching
* Operating system memory management
* Virtual memory systems
* File system caching
* CPU cache management
* Application-level caching (e.g., in-memory caches like Redis or Memcached)
* Content delivery networks (CDNs)
* Mobile app caching
* Game development (e.g., texture caching)
* Multimedia streaming services
* Cloud computing platforms
* E-commerce platforms (e.g., product recommendation caching)
* Social media platforms (e.g., feed caching)
* API response caching
* Machine learning model caching

* **LIFO (Last In, First Out)** - Removes the most recently added items first.

When to use:
* When recent data is less likely to be accessed again.
* When optimizing for scenarios where older data is more relevant.

Real Applications: 
* Undo mechanisms in applications 
* Stack data structures.
* Function call management in programming languages.
* Expression evaluation in compilers.
* Backtracking algorithms.
* Depth-first search algorithms.
* Parsing nested structures.
* Memory management in certain scenarios.
* Web browsing history management.
* Game state management.
* Transaction management in databases.
* Text editor operations (e.g., redo functionality).
* Recursive function calls.
* Expression evaluation in calculators.
* Navigation systems (e.g., backtracking routes).
* Version control systems (e.g., managing changes).
* Cache management in specific applications.
* Data processing pipelines.
* Workflow management systems.
* Task scheduling in certain scenarios.
* Browser tab management.
* Mobile app navigation history.
* Multimedia editing applications (e.g., video editing undo/redo).
* Document editing applications (e.g., version history).
* Email clients (e.g., managing draft versions).
* IDE features (e.g., code navigation history).
* Graphic design software (e.g., layer management).
* 3D modeling applications (e.g., object manipulation history).
* Simulation software (e.g., state management).
* Data analysis tools (e.g., step-by-step data transformation history).
* Scientific computing applications (e.g., iterative method state management).
* Financial software (e.g., transaction rollback features).
* Project management tools (e.g., task history tracking).
* E-learning platforms (e.g., tracking learning progress).



* **LFU (Least Frequently Used)** - Removes the least frequently accessed items first. In case of Tie apply FIFO

When to use:
* When frequently accessed data is more likely to be accessed again.
* When optimizing for access frequency.
* When data access patterns are skewed.
* When historical access patterns are relevant.
* When cache hit rate optimization is a priority.
* When dealing with large datasets with varying access frequencies.
* When cache size is limited and efficient utilization is critical.
* When implementing recommendation systems or personalized content delivery.

Real Applications:
* Web Browsers - Use LRU to manage cached web pages and resources.
* Database Systems - Use LFU to cache frequently accessed query results.
* Content Delivery Networks (CDNs) - Use a combination of LRU and LFU to cache popular content.
* Operating Systems - Use LRU for managing memory pages in virtual memory systems.
* In-Memory Caching Systems - Use LFU in systems like Memcached or Redis to
    cache frequently accessed data.
* Mobile Applications - Use LRU to manage cached data for offline access.
* Video Streaming Services - Use LFU to cache frequently watched videos.
* E-commerce Platforms - Use LFU to cache frequently viewed products.
* Social Media Platforms - Use LRU to cache frequently accessed user profiles and posts.
* API Gateways - Use LFU to cache frequently requested API responses.
* Gaming Applications - Use LRU to manage cached game assets and levels.
* Machine Learning Systems - Use LFU to cache frequently used models or datasets.
* File Systems - Use LRU to manage cached file data for faster access.
* Email Clients - Use LRU to cache frequently accessed emails and attachments.
* News Aggregators - Use LFU to cache frequently read articles.
* Cloud Storage Services - Use LRU to manage cached files for quick access.
* 